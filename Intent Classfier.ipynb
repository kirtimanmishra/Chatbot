{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from random import sample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    intent_label                                        Description\n",
      "12             1  substantially infirmary for centre operating_t...\n",
      "19             1                trump hospital for spunk operation \n",
      "11             1        substantially hospital for spunk operation \n",
      "13             1      substantially infirmary for centre operation \n",
      "21             1              trump infirmary for centre operation \n",
      "9              1       substantially hospital for centre operation \n"
     ]
    }
   ],
   "source": [
    "# read csv file\n",
    "data = pd.read_csv(\"intent1.csv\")\n",
    "print(data.sample(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29,) (3,) (29,) (3,)\n"
     ]
    }
   ],
   "source": [
    "# split dataset into train and test.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data\n",
    "[\"Description\"], data[\"intent_label\"], test_size=3)\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the input using tfidf values.\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf = tfidf.fit(X_train)\n",
    "X_train = tfidf.transform(X_train)\n",
    "X_test = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding for different categories of intents\n",
    "le = LabelEncoder().fit(Y_train)\n",
    "Y_train = le.transform(Y_train)\n",
    "Y_test = le.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIRTIMAN\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# other models like GBM, Random Forest may also be used\n",
    "model = SVC()\n",
    "model = model.fit(X_train, Y_train)\n",
    "p = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.6666666666666666\n",
      "accuracy_score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# calculate the f1_score. average=\"micro\" since we want to calculate score for multiclass.\n",
    "# Each instance(rather than class(search for macro average)) contribute equally towards the scoring.\n",
    "print(\"f1_score:\", f1_score( Y_test, p, average=\"micro\"))\n",
    "print(\"accuracy_score:\",accuracy_score(Y_test, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data = pd.read_csv(\"intent1.csv\")\n",
    "\n",
    "# split data into test and train\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data[\"Description\"], data[\"intent_label\"], test_size=6)\n",
    "\n",
    "# label encoding for different categories of intents\n",
    "le = LabelEncoder().fit(Y_train)\n",
    "Y_train = le.transform(Y_train)\n",
    "Y_test = le.transform(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vectors trained\n"
     ]
    }
   ],
   "source": [
    "# get word_vectors for words in training set\n",
    "X_train = [sent for sent in X_train]\n",
    "X_test = [sent for sent in X_test]\n",
    "\n",
    "word_vecs = Word2Vec(X_train)\n",
    "print(\"Word vectors trained\")\n",
    "\n",
    "# prune each sentence to maximum of 20 words.\n",
    "max_sent_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize input strings\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)\n",
    "\n",
    "# sentences with less than 20 words, will be padded with zeroes to make it of length 20\n",
    "# sentences with more than 20 words, will be pruned to 20.\n",
    "x = pad_sequences(sequences, maxlen=max_sent_len)\n",
    "X_test = pad_sequences(sequences_test, maxlen=max_sent_len)\n",
    "    \n",
    "# 100 is the size of wordvec.\n",
    "embedding_matrix = np.zeros((vocab_size + 1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KIRTIMAN\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings done\n"
     ]
    }
   ],
   "source": [
    "# make matrix of each word with its word_vectors for the CNN model. \n",
    "# so each row of a matrix will represent one word. There will be a row for each word in\n",
    "# the training set\n",
    "for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vector = word_vecs[word]\n",
    "        except:\n",
    "            embedding_vector = None\n",
    "            if embedding_vector is not None:\n",
    "            \tembedding_matrix[i] = embedding_vector\n",
    "print(\"Embeddings done\")\n",
    "vocab_size = len(embedding_matrix)\n",
    "\n",
    "# CNN model requires multiclass labels to be converted into one hot ecoding.\n",
    "# i.e. each column represents a label, and will be marked one for corresponding label.\n",
    "y = to_categorical(np.asarray(Y_train))\n",
    "\n",
    "embedding_layer = Embedding(vocab_size,\n",
    "                                100,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sent_len,\n",
    "                                trainable=True)\n",
    "sequence_input = Input(shape=(max_sent_len,), dtype='int32')\n",
    "\n",
    "# stack each word of a sentence in a matrix. So each matrix represents a sentence.\n",
    "# Each row in a matrix is a word(Word Vector) of a sentence.\n",
    "embedded_sequences = embedding_layer(sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - simplified convolutional neural network\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 20, 100)           2800      \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 17, 128)           51328     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               51300     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 105,731\n",
      "Trainable params: 105,731\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build the Convolutional model.\n",
    "l_cov1 = Conv1D(128, 4, activation='relu')(embedded_sequences)\n",
    "l_pool1 = MaxPooling1D(4)(l_cov1)\n",
    "l_flat = Flatten()(l_pool1)\n",
    "hidden = Dense(100, activation='relu')(l_flat)\n",
    "preds = Dense(len(y[0]), activation='softmax')(hidden)\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',optimizer='Adam')\n",
    "\n",
    "print(\"model fitting - simplified convolutional neural network\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "26/26 [==============================] - 0s 355us/step - loss: 0.6256\n",
      "Epoch 2/20\n",
      "26/26 [==============================] - 0s 366us/step - loss: 0.6255\n",
      "Epoch 3/20\n",
      "26/26 [==============================] - 0s 391us/step - loss: 0.6255\n",
      "Epoch 4/20\n",
      "26/26 [==============================] - 0s 367us/step - loss: 0.6254\n",
      "Epoch 5/20\n",
      "26/26 [==============================] - 0s 231us/step - loss: 0.6253\n",
      "Epoch 6/20\n",
      "26/26 [==============================] - 0s 520us/step - loss: 0.6253\n",
      "Epoch 7/20\n",
      "26/26 [==============================] - 0s 401us/step - loss: 0.6252\n",
      "Epoch 8/20\n",
      "26/26 [==============================] - 0s 242us/step - loss: 0.6251\n",
      "Epoch 9/20\n",
      "26/26 [==============================] - 0s 390us/step - loss: 0.6251\n",
      "Epoch 10/20\n",
      "26/26 [==============================] - 0s 360us/step - loss: 0.6250\n",
      "Epoch 11/20\n",
      "26/26 [==============================] - 0s 219us/step - loss: 0.6249\n",
      "Epoch 12/20\n",
      "26/26 [==============================] - 0s 414us/step - loss: 0.6249\n",
      "Epoch 13/20\n",
      "26/26 [==============================] - 0s 334us/step - loss: 0.6248\n",
      "Epoch 14/20\n",
      "26/26 [==============================] - 0s 452us/step - loss: 0.6248\n",
      "Epoch 15/20\n",
      "26/26 [==============================] - 0s 442us/step - loss: 0.6247\n",
      "Epoch 16/20\n",
      "26/26 [==============================] - 0s 332us/step - loss: 0.6246\n",
      "Epoch 17/20\n",
      "26/26 [==============================] - 0s 330us/step - loss: 0.6246\n",
      "Epoch 18/20\n",
      "26/26 [==============================] - 0s 446us/step - loss: 0.6245\n",
      "Epoch 19/20\n",
      "26/26 [==============================] - 0s 303us/step - loss: 0.6244\n",
      "Epoch 20/20\n",
      "26/26 [==============================] - 0s 412us/step - loss: 0.6244\n",
      "accuracy_score: 0.6666666666666666\n",
      "f1_score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(x, y, epochs=20, batch_size=128)\n",
    "\n",
    "#get scores and predictions.\n",
    "p = model.predict(X_test)\n",
    "p = [np.argmax(i) for i in p]\n",
    "score_cnn = f1_score(Y_test, p, average=\"micro\")\n",
    "print(\"accuracy_score:\",accuracy_score(Y_test, p))\n",
    "print(\"f1_score:\", score_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
