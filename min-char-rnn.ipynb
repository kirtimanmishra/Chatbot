{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 50015 characters, 76 unique.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('sample_text.txt', 'r').read() # should be simple plain text file\n",
    "data = data.split()     #  a word-level language model\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"\n",
    "  inputs,targets are both list of integers.\n",
    "  hprev is Hx1 array of initial hidden state\n",
    "  returns the loss, gradients on model parameters, and last hidden state\n",
    "  \"\"\"\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  loss = 0\n",
    "  # forward pass\n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "  # backward pass: compute gradients going backwards\n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    dy = np.copy(ps[t])\n",
    "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    dby += dy\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "    dbh += dhraw\n",
    "    dWxh += np.dot(dhraw, xs[t].T)\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "    dhnext = np.dot(Whh.T, dhraw)\n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " 2 \"4o\n",
      "\"uV[eA$GO6râOitâ1âmT€?nhdEondL.\n",
      "CD%Psâ;6ox'd:.q\"F%3€tz1c](PNiTtBoP](Oj-cCEz6bfxq\n",
      ";'pkk0ww-â3frfP”oywWcHE(P0H :?1xxwuD3%YUeP LxRB0NL€4?G4AYFe3ml(\n",
      "H\n",
      "€;NOSNH[yEwAv\n",
      "P:O”zw6$n3,we_%L;KzH\"2BFoKNK.”8lD \n",
      "----\n",
      "iter 0, loss: 108.268326\n",
      "----\n",
      "  os uirg.a'rrri ue ooa suisraro l u u e ps d y.uuauf  rusuamy iouI iiri ue   r id tmeua uru' uuuu.o  ooauuu'um rii'i s'hyTmaur nbouu  sh e souueiyi eustte'.'utrbte'usiai,teituiorr   tuii.  m nisrm egu \n",
      "----\n",
      "iter 100, loss: 107.433887\n",
      "----\n",
      " ade  vefke  fafif  dilsrerrrr s lteae seif a et  Ge laoey  d et  pesr   lep  celespw i utrkaffe  iiipk  a m eye ro t   eepoa  reve soaoears. eo  s cu t e e rafoa   ur sfe d ra d p s  y e e f w ar r en \n",
      "----\n",
      "iter 200, loss: 104.648083\n",
      "----\n",
      "   eere nuthde tt en teus  te aTin toinncer otennatrat ra eant gounoulard toa here le k.ida samisule noto t y kes bnlre we uf dar. an po zd ty try. doc d talinar  dew sb s bndudrsfdfirde iid likaneu t  \n",
      "----\n",
      "iter 300, loss: 101.644561\n",
      "----\n",
      " n splengemd thxdy hung isy dne goutdiTabror t io tout ve be ppnfo tu ge t cuoqrefilatoilt p.ptnfpge ir  tetoep\"urt t we oou te  , itawanAeeenyt t” c.pf ofrawu, t  tfrknt thth  g gut ieott ke t bn un c \n",
      "----\n",
      "iter 400, loss: 98.445724\n",
      "----\n",
      " in Adusnm bamipset th w bath woss gory wehe anoiro sto Bynlnniegm thena eosvendin,udhen opwm ko get to Aa fouwsene ons  thm ukatnkisorawiaf whe yota  io seuaes gonyke tolg sonreng iagost poe woewthing \n",
      "----\n",
      "iter 500, loss: 95.432470\n",
      "----\n",
      " ams Tou sostorp ohe tee waat rihr gourenonab ldrms rhf yak pof sad of th f sr stofthoto toonotrinwemersfyfot yotf rontcupe hane ad if tiopethanereisxt ioe borerfnh ird e safi zhf ar, ro szorn rsretiwa \n",
      "----\n",
      "iter 600, loss: 92.378583\n",
      "----\n",
      " ern f'n z,reutyerertdeinholarist th nitle fhat andrf one le ,rer  the't'te tort al to he theri g1t the tary ihe ber whe lorere thero ct ther r, pre tart pol mod ols ate yon ho  iy ove pre fa g youn th \n",
      "----\n",
      "iter 700, loss: 89.502345\n",
      "----\n",
      " ley e s npoo'rcunurelt poncle auned cuve yledrlsn lin  oh ba bf tut i..ny th rounsorxuan cils i_re tiunyo wxfonwbenweornthe weytllokicen imco cunlpanius ru eolc be clervily srothin[ucust areyaufonnaph \n",
      "----\n",
      "iter 800, loss: 86.761142\n",
      "----\n",
      "  s. yoro lorod yerl anre n nicde todO he d ood s ch to eorassee. the ueke turl bot bard  aadspre to , dempour Aarpndoure so gorh Yoad s ny qasse dun to xdate'se  oa co. tr s, hois serp sral co teco mo \n",
      "----\n",
      "iter 900, loss: 84.220711\n",
      "----\n",
      " et ro e , wiokeeg au'replst's is  on tor thor boheor hifsas. om ror ere tor it 't arithe searemlrrat you bove tohlicars thad in at.Wre ed in't e peyins, angrpls mrt rst wot by.ms gr be yoe git why ke  \n",
      "----\n",
      "iter 1000, loss: 81.849042\n",
      "----\n",
      "  insinnd int inimniny oi ileoren werlingo kunin,iconrthasiankgaynn mou cave hous afo this iiging bamanifhave wa] fancfntumerdoctoThinpre con rove cfilm hfalcn Thongiged. ofing. to bentumikdeont pcinin \n",
      "----\n",
      "iter 1100, loss: 79.691064\n",
      "----\n",
      " s cout If Bc ouridon fsurne 'o sr bane shadpree 'f erd you b checcistou I fo rauved y dthls ra mohe me wof a the d iW deenif,  hutu the moupd foon pa wre tt boss ard su suthf suonrerr hh madr what Iny \n",
      "----\n",
      "iter 1200, loss: 77.601477\n",
      "----\n",
      " sren won' bet an welan fso onlwake thad yak in oud woe if aoilo if at w oonldon ar mup thellg lo theby thet it a goul at to  ove, boul  fulg to pom axher a lo mon se ans youn u durs, put chave limm th \n",
      "----\n",
      "iter 1300, loss: 75.694603\n",
      "----\n",
      " soopre angald psogepeo thehthar vuspanda Io lust s Saly fom of sol theteme anve feos Bere Thens on do lavewart ateress. are s, copliat cell fonme eans thedelemo of bevee mospane thee meof aracone of s \n",
      "----\n",
      "iter 1400, loss: 73.874551\n",
      "----\n",
      " speng there hor, hane tofh th wor't on the stobll devang snving woheve giwe thet hhat whevers. ro] yoe greedby tew rave the thencesinves. heos thave unge wow the bett bithrned thevlnherereis a oond ad \n",
      "----\n",
      "iter 1500, loss: 72.120262\n",
      "----\n",
      " dree sem nekeet be bes gop. zobe cakn'won com wuvy bompnheyoulesthain tonlo wond Ifs move vey wtasirftinin'fouke't aly ung af boug bit yout mon wout lindeing beveb toedon cond coole youn  gato zoing a \n",
      "----\n",
      "iter 1600, loss: 70.441499\n",
      "----\n",
      " ore Sort decher€ibm thi thoathe nhic meab”esewople peotfitheorninv an ieusind foveogee tot ingerout tainimey'upheprtty weren omoondering bill in aneinin forioglirohemachemcart is inulitue winis pifini \n",
      "----\n",
      "iter 1700, loss: 68.899861\n",
      "----\n",
      " g. not  y youd Thuali-isupuncau'cm moul exs The leing the siol wiot the tund uf mon't er'w you woc fous ould they liku you, yout munm iluctulnr is one ho plothan sicin thinely sais you id thucr talpeu \n",
      "----\n",
      "iter 1800, loss: 67.514238\n",
      "----\n",
      "  'll in thectobe suke bikidere cath. yous yopl them mase rithe sofald amd af mewer wike otir, roth.darer wiks ssoud Ikup eu amy inkey it wort ay inom yoarpinr you eisu 3y es menseing you you'de you're \n",
      "----\n",
      "iter 1900, loss: 66.169609\n",
      "----\n",
      " h to the yould Inile ro me juting you'le Iig them tare ro  \"y the to se sul fot  ont in conenowsalg ex sa gelare socfinge the you igne coy't leeres youst the de s ons is dut one to fulle ldupst grous  \n",
      "----\n",
      "iter 2000, loss: 64.904767\n",
      "----\n",
      " e €an't haasitomu_illathat uispateres. s dead Whad bes consters af reum rout so no, you're s iine wioheris. ceculd. soinga ooidempme the camey in fastoun,uy. Ao homiiy the win doisinerin toull thourr  \n",
      "----\n",
      "iter 2100, loss: 63.956604\n",
      "----\n",
      "  of dare ale of,e won't ynat m3uld wead culy ay lade ll. more tif und, allis pas notn mo to mize se and wilit  abl ardate goaly lord ta lake a aarce of  The lealy €ncare roll hey whasp on youp f io se \n",
      "----\n",
      "iter 2200, loss: 63.032185\n",
      "----\n",
      " rrcidurectvund Ben, They iTcenmyors tameln an'rs yble then dorly iBct le ldems The ju de thy excenle thethinlt expe, n inet beorectey dost of a det noy fongun u dersa teas pro oulere a gran meninicing \n",
      "----\n",
      "iter 2300, loss: 62.047382\n",
      "----\n",
      " g thed thingreree”on. Ah whause hhat prath go angut st.renmereleleltreald thedesting th stet amiond sard ”alvwone uld. wich yout hill of finderumthe topses jucpen Beve s,droll to fellcoYing, ans llitu \n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-f7e4940bc02e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m   \u001b[1;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m   \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m   \u001b[0msmooth_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.999\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'iter %d, loss: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msmooth_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# print progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-b6e72c13c965>\u001b[0m in \u001b[0;36mlossFun\u001b[1;34m(inputs, targets, hprev)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mdWhy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mdby\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mdh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdhnext\u001b[0m \u001b[1;31m# backprop into h\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mdhraw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdh\u001b[0m \u001b[1;31m# backprop through tanh nonlinearity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mdbh\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdhraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "  \"\"\" \n",
    "  sample a sequence of integers from the model \n",
    "  h is memory state, seed_ix is seed letter for first time step\n",
    "  \"\"\"\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  x[seed_ix] = 1\n",
    "  ixes = []\n",
    "  for t in range(n):\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    y = np.dot(Why, h) + by\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[ix] = 1\n",
    "    ixes.append(ix)\n",
    "  return ixes\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while True:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "  if n % 100 == 0:\n",
    "    sample_ix = sample(hprev, inputs[0], 200)\n",
    "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "    print ('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "  if n % 100 == 0: print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "  p += seq_length # move data pointer\n",
    "  n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
